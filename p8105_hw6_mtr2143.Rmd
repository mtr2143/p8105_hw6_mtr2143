---
title: "Homework 6"
author: Matthew T. Russell
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(modelr)
library(knitr)

theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE
)
```

# Problem 1

```{r, read in birthweight data and see if there are any NA values}
birthweight_data <- read_csv("https://www.p8105.com/data/birthweight.csv")

birthweight_missing <- map_df(birthweight_data, ~sum(is.na(.))) %>% 
  pivot_longer(
    cols = babysex:wtgain,
    names_to = "variable", 
    values_to = "num_missing"
  )

birthweight_missing %>% kable()
```

From the table above, we can see that none of the variables contain any NA values. 

```{r, see if frace variable has any unknown}
birthweight_data %>% 
  group_by(frace) %>% 
  summarize(
    total = n()
  ) %>% 
  kable()
```

Per the data description, we know that the variable correspond to the father's race has a value for observations where the value is unknown (`frace == 9`), However, from the table above, we can see that there aren't any observations that have this value. 

Now that we know there aren't any missing values, we can fit a model. 

```{r, tidying birthweight data}
birthweight_df <-
  birthweight_data %>% 
  mutate(
    babysex = ifelse(babysex == 1, "male", "female") %>% 
      forcats::as_factor() %>% 
      relevel(ref = "male"),
    frace = case_when(
      frace == 1 ~ "white", 
      frace == 2 ~ "black", 
      frace == 3 ~ "asian", 
      frace == 4 ~ "puerto rican", 
      frace == 8 ~ "other"
    ) %>% 
      forcats::as_factor() %>% 
      relevel(ref = "other"),
    malform = ifelse(malform == 0, "absent", "present") %>% 
      forcats::as_factor() %>% 
      relevel(ref = "absent"), 
    mrace = case_when(
      mrace == 1 ~ "white", 
      mrace == 2 ~ "black", 
      mrace == 3 ~ "asian", 
      mrace == 4 ~ "puerto rican"
    ) %>% 
      forcats::as_factor() %>% 
      relevel(ref = "puerto rican")
  )
```

```{r, regression model}
bwt_fit <- lm(bwt ~ fincome + frace + gaweeks + mrace + momage + smoken, data = birthweight_df)

bwt_fit %>% 
  broom::tidy() %>% 
  select(term, estimate, p.value) %>% 
  mutate(
    term = str_replace(term, "^frace", "frace: "), 
    term = str_replace(term, "^mrace", "mrace: ")
  ) %>% 
  knitr::kable(digits = 3)
```

I chose to run a model where baby's birthweight (g) is predicted by family income, father's race, gestational age at delivery (weeks), mother's race, mother's age at delivery (years), and average number of cigarettes smoked per day during pregnancy. The goal of this model was to predict birthweight based on known biological factors that influence birthweight (gestational age and number of cigarettes smoked during pregnancy) with social factor (family income and father and mother's race). 

```{r, plot of model residuals against fitted values, fig.dim=c(7,5)}
birthweight_df %>% 
  modelr::add_predictions(bwt_fit) %>% 
  modelr::add_residuals(bwt_fit) %>% 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point(alpha = .3) +
  xlab("Predicted Value") + ylab("Residual")
```

The above plot shows the residuals vs the predicted values. For the assumption of linearity to be hold, we'd expect the points to be distributed randomly around y = 0, but that is not true of this model. There also appears to be outliers present in our data given the points that are far outside the main clustering of where the predicted value is between 2500 and 2500 and the residual is 0. 

```{r, model comparison, fig.dim=c(7,5)}
set.seed(32694)

cv_df <- 
  birthweight_df %>% 
  crossv_mc(., 100)

cv_df <-
  cv_df %>% 
  mutate(
    my_mod = map(train, ~lm(bwt ~ fincome + frace + gaweeks + mrace + momage + smoken, data = .x)),
    main_mod = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)), 
    inter_mod = map(train, ~lm(bwt ~ bhead * babysex * blength, data = .x))
  ) %>% 
  mutate(
    rmse_my_model = map2_dbl(my_mod, test, ~rmse(model = .x, data = .y)), 
    rmse_main_effects = map2_dbl(main_mod, test, ~rmse(model = .x, data = .y)),
    rmse_interaction = map2_dbl(inter_mod, test, ~rmse(model = .x, data = .y)),
  )

cv_df %>% 
  select(
    starts_with("rmse")
    ) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(
    model = fct_inorder(model)
    ) %>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin() +
  xlab("Model") + ylab("RMSE")
```

From the plot above, we can see the model I fit is by far the worst. Judging by the RMSE, the model with the three-way interaction is the best predictor of a baby's birthweight. However, given that a model with interaction is difficult to describe, I'd argue that the main effect model is the best model.

# Problem 2

```{r, load weather df}
weather_df <- 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

```{r, bootstrapping}
set.seed(12551)

boot_straps <- 
  weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x)),
    model_est = map(models, broom::glance), 
    model_coef = map(models, broom::tidy)
  ) %>% 
  select(-strap, -models) %>%
  unnest(model_est) %>% 
  select(.id, model_coef, r.squared) %>% 
  unnest(model_coef) %>% 
  select(.id, term, estimate, r.squared) %>% 
  pivot_wider(
    names_from = "term", 
    values_from = "estimate", 
    names_repair = "universal"
  ) %>% 
  mutate(
    log_b0_b1 = log(.Intercept.*tmin)
  ) %>% 
  select(.id, r.squared, log_b0_b1)

```

